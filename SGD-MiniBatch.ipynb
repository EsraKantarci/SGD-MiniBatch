{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent and Mini-Batch Gradient Descent Modifications in HW-3\n",
    "### Esra KantarcÄ± - 20160808023\n",
    "\n",
    "**Details of the task:**\n",
    "\n",
    "*Hi Everyone, for the previous task you created multi layer perceptron to classify rice data. During the train, you used backpropagation method to get update information and updated the weights. The way you updated the weights is called gradient descent and you used whole dataset as your reference is called batch gradient descent. You already implemented that. But in real-life scenarios generally we can't fit all of the training data into memory. Therefore, we use methods called stochastic gradient descent and mini-batch gradient descent. These methods use single or given size(usually referred to as batch size) samples to update the network.*\n",
    "\n",
    "*For this task, we expect you to implement SGD and Mini-Batch Gradient Descent into your multilayer perceptron you created last week and test them with rice data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: \n",
    "\n",
    "So, as the first, everything we used in the Homework-3 stays the same. Therefore, primitive normalization and taking test and training values are still the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Osmancik' 'Osmancik' 'Osmancik' ... 'Osmancik' 'Cammeo' 'Cammeo']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\esrac\\\\Downloads\")\n",
    "df = pd.read_csv('Ricetrain.csv', header=None)\n",
    "df_shuffled = df.sample(frac=1)\n",
    "y = df_shuffled.iloc[0:,7].values\n",
    "y = np.where(y == \"Osmancik\", -1 ,1)\n",
    "X = df_shuffled.iloc[0:, 0:7].values\n",
    "\n",
    "df_test = pd.read_csv('Ricetest.csv', header=None)\n",
    "df_shuffled_test = df_test.sample(frac=1)\n",
    "\n",
    "y_test = df_shuffled_test.iloc[0:,7].values\n",
    "print(y_test)\n",
    "y_test = np.where(y_test == \"Osmancik\", -1 ,1)\n",
    "X_test = df_shuffled_test.iloc[0:, 0:7].values\n",
    "\n",
    "X_normed = X / X.max(axis=0)\n",
    "X_test_normed = X_test / X_test.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Predictors shape (2667, 7)\n",
      "Test - Predictors shape (1143, 7)\n",
      "Train - Target shape (2667,)\n",
      "Test - Target shape (1143,)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_normed\n",
    "x_test =X_test_normed\n",
    "y_train = y\n",
    "y_test = y_test\n",
    "#\n",
    "print('Train - Predictors shape', x_train.shape)\n",
    "print('Test - Predictors shape', x_test.shape)\n",
    "print('Train - Target shape', y_train.shape)\n",
    "print('Test - Target shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from HW-3 \n",
    "\n",
    "The neural network from the HW-3 uses a simple gradient descent algorithm.\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "Let's recall what is gradient descent from the HW-3. In simple gradient descent, which we use in the backpropagation phase for the weight update, we simply try to reach to the global minimum by using training samples. We already explained the backbone in the HW-3, therefore I am only adding 2 representative graphics for recalling purposes. \n",
    "\n",
    "\n",
    "Here is the neural network from the HW-3 and we will implement Stocastic and Mini-Batch versions afterwards. As you expect, we will only modify the gradient function and just for the readibility purposes, we will add respective training versions. (But we could override it as well.)\n",
    "\n",
    "What we had used in our initial gradient descent implementation was batch gradient descent. It updates weights after each training epoch. So, all the iterations cycle uses all the training examples for updates. It is stable, straightforward application and easy to process paralelly. It is kind of slow but very effective. Let's take a look at Mini-Batch and Stocastic versions.\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Since the word is a little bit long, I will call this word SGD. \n",
    "It is a gradient descent algorithm which updates weights by every training sample again and again. For example, at the first you got 1 sample, then 2, you do not calculate weights immediately but you calculate them at each iteration. This update model can be slower than normal and finding correct weight values takes more time and it gets updated much often.\n",
    "\n",
    "The advantages of SGD are the immediate performance update by every sample and easily applicable for the algorithms. But it also has disadvantages. It is computationally more complex therefore more expensive. It takes time and since there are too many updates, it does not always end up with the best minimum value.\n",
    "\n",
    "In order to do this, I had only added \"iterations\" variable, so inside 2 for-loops, on each sample, the weights will get updated. You can see the similar implementations in the references below.\n",
    "\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent is a cool trick to make batch gradient descent more robust to avoid local minimas and faster computations using slides.\n",
    "\n",
    "It takes a number of samples (shuffled) and calculates weights on that iteration cycle. So, batch gradient descent was doing the same for the number of training sample, in that case, mini-batch does it for a fixed number of the shuffled training samples at each epoch.\n",
    "\n",
    "Batch size itself is a hyperparameter. Small values converges quickly but has more error rate. Large values converges slowly but with ends up with more accurate predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, inputLayer=3, hiddenLayers=[3, 5], lastLayer=2):\n",
    "       \n",
    "\n",
    "        self.inputLayer = inputLayer\n",
    "        self.hiddenLayers = hiddenLayers\n",
    "        self.lastLayer = lastLayer\n",
    "        layers = [inputLayer] + hiddenLayers + [lastLayer]\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        \n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        \n",
    "    def forwardProp(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        \n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "            self.activations[i + 1] = activations\n",
    "        return activations\n",
    "\n",
    "    def backProp(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            activations = self.activations[i+1]\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            \n",
    "    def gradientDescent(self, learningRate=0.4):\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "             \n",
    "                \n",
    "    def train(self, inputs, targets, epochs=50, learningRate=0.2):\n",
    "        #epochs and learning rate are hyperparameters\n",
    "        #training is similar to perceptron but needs forward feed\n",
    "        #backpropagation and gradient descent as well\n",
    "        for i in range(epochs):\n",
    "            sumErrors = 0\n",
    "            #we will loop through the inputs and its indexes\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "                #activating the layer by forward propagation\n",
    "                output = self.forwardProp(input)\n",
    "                #calculating the error \n",
    "                error = target - output\n",
    "                \n",
    "                self.backProp(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights)\n",
    "                self.gradientDescent(learningRate)\n",
    "                sumErrors += self._mse(target, output)\n",
    "                #we can report using sumErrors at each epoch to \n",
    "                #check the error, but I did not display it as the video suggested.\n",
    "\n",
    "        print(\"Trained the model.\")\n",
    "            \n",
    "                \n",
    "                \n",
    "    def miniBatchGradientDescent(self, learningRate=0.4):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "            \n",
    "    def miniBatchTrain(self, inputs, targets, epochs=50, learningRate=0.2, batchSize=30):\n",
    "        #epochs and learning rate are hyperparameters\n",
    "        #training is similar to perceptron but needs forward feed\n",
    "        #backpropagation and gradient descent as well\n",
    "        for i in range(epochs):\n",
    "            sumErrors = 0\n",
    "            #we will loop through the inputs and its indexes\n",
    "            X, y = shuffle(inputs, targets)\n",
    "            x_random = X[:batchSize]\n",
    "            y_random = y[:batchSize]\n",
    "            inputs = x_random\n",
    "            targets = y_random\n",
    "        \n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "                #activating the layer by forward propagation\n",
    "                output = self.forwardProp(input)\n",
    "                #calculating the error \n",
    "                error = target - output\n",
    "                \n",
    "                self.backProp(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights)\n",
    "                self.miniBatchGradientDescent(learningRate)\n",
    "                sumErrors += self._mse(target, output)\n",
    "                #we can report using sumErrors at each epoch to \n",
    "                #check the error, but I did not display it as the video suggested.\n",
    "\n",
    "        print(\"Trained the model with minibatch gradient descent with batch size:\", batchSize )\n",
    "    \n",
    "             \n",
    "    def stochasticGradientDescent(self, learningRate=0.4,iterations=10):\n",
    "        for i in range(iterations):\n",
    "            for j in range(len(self.weights)):\n",
    "                weights = self.weights[j]\n",
    "                derivatives = self.derivatives[j]\n",
    "                weights += derivatives * learningRate           \n",
    "    \n",
    "    def sgdTrain(self, inputs, targets, epochs=50, learningRate=0.2, iterations=10):\n",
    "        #epochs and learning rate are hyperparameters\n",
    "        #training is similar to perceptron but needs forward feed\n",
    "        #backpropagation and gradient descent as well\n",
    "        for i in range(epochs):\n",
    "            sumErrors = 0\n",
    "            X, y = shuffle(inputs, targets)\n",
    "            inputs = X\n",
    "            targets = y\n",
    "\n",
    "            #we will loop through the inputs and its indexes\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "                #activating the layer by forward propagation\n",
    "                output = self.forwardProp(input)\n",
    "                #calculating the error \n",
    "                error = target - output\n",
    "                \n",
    "                self.backProp(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights)\n",
    "                self.stochasticGradientDescent(learningRate)\n",
    "                sumErrors += self._mse(target, output)\n",
    "                #we can report using sumErrors at each epoch to \n",
    "                #check the error, but I did not display it as the video suggested.\n",
    "\n",
    "        print(\"Trained the model with stochastic gradient descent\")\n",
    "    \n",
    "    def classification(self,output): \n",
    "        return np.where(output/np.average(output) < 1, 1, -1)\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        return np.average((target - output) ** 2)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "The original code used batch style gradient descent. The overall accuracy is between 84%-87% stable. It takes on average 4 seconds.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "The SGD overall accuracy is between 81%-85% and stable. It takes on average 8 seconds. The number of iterations do not have big impact on the time complexity. Shuffling affects the prediction results.\n",
    "\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "Batch size really affects the result accuracy. The accuracy can be lower than 65% according to shuffle and the low batch size. But if the batch size, which is a hyperparameter, is selected right, the accuracy can be higher than stochastic, too. But the dramatic improvement is on the timecomplexity: it only takes 0.05-0.08 seconds to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model.\n",
      "--- 4.763277530670166 seconds ---\n",
      "[[-1 -1 -1 ...  1  1  1]]\n",
      "Accuracy is: 85.03937007874016 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "nn.train(items, targets, 30, 0.1)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model with stochastic gradient descent\n",
      "--- 8.56310510635376 seconds ---\n",
      "[[-1 -1 -1 ...  1  1  1]]\n",
      "Accuracy is: 82.1522309711286 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "iterations = items.shape[0]\n",
    "nn.sgdTrain(items, targets, 30, 0.1, iterations)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model with minibatch gradient descent with batch size: 10\n",
      "--- 0.027922630310058594 seconds ---\n",
      "[[-1  1  1 ...  1  1  1]]\n",
      "Accuracy is: 62.467191601049876 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "start_time = time.time()\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "nn.miniBatchTrain(items, targets, 30, 0.1, 10)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model with minibatch gradient descent with batch size: 32\n",
      "--- 0.07283854484558105 seconds ---\n",
      "[[-1 -1 -1 ...  1 -1  1]]\n",
      "Accuracy is: 85.8267716535433 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "nn.miniBatchTrain(items, targets, 30, 0.1, 32)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model with minibatch gradient descent with batch size: 24\n",
      "--- 0.05485367774963379 seconds ---\n",
      "[[-1 -1 -1 ...  1 -1  1]]\n",
      "Accuracy is: 86.43919510061242 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "nn.miniBatchTrain(items, targets, 30, 0.1, 24)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model with stochastic gradient descent\n",
      "--- 8.339909076690674 seconds ---\n",
      "[[-1 -1 -1 ...  1  1  1]]\n",
      "Accuracy is: 85.12685914260717 %.\n"
     ]
    }
   ],
   "source": [
    "# Adding datasets for training the network\n",
    "items = X_normed\n",
    "targets = y\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "#Creating the 1 hidden layered network with 5 neurons inside\n",
    "#we want to get the first layer's size as the number of the features\n",
    "inputSize = items.shape[1]\n",
    "nn = ModifiedNeuralNetwork(inputSize, [5], 1)\n",
    "iterations = 100\n",
    "nn.sgdTrain(items, targets, 30, 0.1, iterations)\n",
    "\n",
    "#After training, adding the test data\n",
    "input = X_test_normed\n",
    "target = y_test\n",
    "\n",
    "#Predictions and classifications.\n",
    "output = nn.forwardProp(input)\n",
    "result = nn.classification(output).T\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(result)\n",
    "\n",
    "dataset = pd.DataFrame({'Predicted': result[len(result)-1], 'Actual': target})\n",
    "dataset['Score_diff'] = dataset['Predicted'].sub(dataset['Actual'], axis = 0) \n",
    "dataset[\"Score_diff\"] = np.where(dataset[\"Score_diff\"] == 0 , 0 , 1)\n",
    "dataset\n",
    "column_sums = dataset.sum(axis=0)\n",
    "accuracy = 1 - (column_sums[2] / target.shape[0])\n",
    "print(\"Accuracy is:\", accuracy*100, \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "In conclusion, when we consider the trade-offs such as time complexity and accuracy, the best results are given by mini-batch approach on the fastest way. All the variants are good optimizers, but my preference would be on 32-64 window-sized mini-batch implementation.\n",
    "\n",
    "Missing local minima and noise are also problematic cases in mini-batch algorithm, therefore the safest approach is using simple gradient descent for no-risk lovers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography \n",
    "\n",
    "\n",
    "- Imad Dabbura, Gradient Descent Algorithm and Its Variants, https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3\n",
    "\n",
    "- Artem Oppenmann, Stochastic Batch and Mini Batch Gradient Descent, https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5\n",
    "\n",
    "- Hamza Mahmood, Gradient Descent, https://towardsdatascience.com/gradient-descent-3a7db7520711\n",
    "\n",
    "- Sushrut Bidwai, Week 10 Machine Learning, http://sushrutbidwai.com/week-10---machine-learning\n",
    "\n",
    "- Jasmeet Singh, Implementing SGD from Scratch, https://towardsdatascience.com/implementing-sgd-from-scratch-d425db18a72c\n",
    "\n",
    "- Jason Brownlee, A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size, \n",
    "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "\n",
    "- Jason Brownlee, How to Code Neural Network with Backpropagation in Python (from scratch): https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "- Valerio Velardo, Training a Neural Network 6-7-8: https://www.youtube.com/watch?v=Z97XGNUUx9o\n",
    "\n",
    "- Github, mattm, Simple Neural Network: https://github.com/mattm/simple-neural-network\n",
    "\n",
    "- Github, emilwallner, Deep Learning From Scratch: https://github.com/emilwallner/Deep-Learning-From-Scratch\n",
    "\n",
    "- Github, KarnageKnight, Neural Network with n Hidden Layers: https://github.com/KarnageKnight/Neural-Network-with-n-hidden-layers\n",
    "\n",
    "- Github, JGuymont, Numpy Multilayer Perceptron: https://github.com/JGuymont/numpy-multilayer-perceptron\n",
    "\n",
    "- Github, musikalkemist, Deep Learning for Audio with Python (my main code reference): https://github.com/musikalkemist/DeepLearningForAudioWithPython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
